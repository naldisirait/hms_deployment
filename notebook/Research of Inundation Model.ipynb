{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c9b7f02-07ed-4fd3-b667-e3e1b94b75fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA RTX A5000\n",
      "  Total Memory: 24564.00 MB\n",
      "  Free Memory: 20830.12 MB\n",
      "  Used Memory: 3733.88 MB\n",
      "\n",
      "GPU 1: NVIDIA RTX A5000\n",
      "  Total Memory: 24564.00 MB\n",
      "  Free Memory: 522.12 MB\n",
      "  Used Memory: 24041.88 MB\n",
      "\n",
      "GPU 2: Quadro RTX 5000\n",
      "  Total Memory: 16384.00 MB\n",
      "  Free Memory: 16114.38 MB\n",
      "  Used Memory: 269.62 MB\n",
      "\n",
      "GPU 3: Quadro RTX 5000\n",
      "  Total Memory: 16384.00 MB\n",
      "  Free Memory: 15700.38 MB\n",
      "  Used Memory: 683.62 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pynvml\n",
    "\n",
    "def list_gpus_memory():\n",
    "    pynvml.nvmlInit()\n",
    "    device_count = pynvml.nvmlDeviceGetCount()\n",
    "\n",
    "    for i in range(device_count):\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "        info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "        name = pynvml.nvmlDeviceGetName(handle)\n",
    "        print(f\"GPU {i}: {name}\")\n",
    "        print(f\"  Total Memory: {info.total / (1024**2):.2f} MB\")\n",
    "        print(f\"  Free Memory: {info.free / (1024**2):.2f} MB\")\n",
    "        print(f\"  Used Memory: {info.used / (1024**2):.2f} MB\")\n",
    "        print()\n",
    "    pynvml.nvmlShutdown()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    list_gpus_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25cdb495-7478-43dc-8066-b7a0517ea2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Set the seed for generating random numbers.\n",
    "\n",
    "    Args:\n",
    "        seed (int): The seed value to use.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True  # for reproducibility\n",
    "    torch.backends.cudnn.benchmark = False  # for reproducibility\n",
    "\n",
    "# set seed\n",
    "set_seed(42)\n",
    "gpu_idx = 0\n",
    "device = torch.device(f\"cuda:{gpu_idx}\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "torch.cuda.set_device(gpu_idx)\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be38043d-6337-4143-a76e-6bafa4ba5c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Open the file containing the pickled data\n",
    "with open('dataset ml2 max depth.pkl', 'rb') as file:\n",
    "    # Load data from file\n",
    "    dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9d3a76e-988f-43b5-8a04-b6016a0660e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_max_depth <class 'list'>\n",
      "debit <class 'list'>\n",
      "max_depth <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "for key in dataset:\n",
    "    print(key, type(dataset[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bff8fc0d-6cad-4e21-a539-ee8592c1a0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(dataset['debit'])\n",
    "y = np.array(dataset['max_depth'])\n",
    "y[np.isnan(y)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "937b58c5-1280-40e3-8b92-a948455b1e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37, 72), (37, 3078, 2019))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8f61e61-1737-4647-bae7-42ff7c4a3d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 72) (7, 72) (30, 3078, 2019) (7, 3078, 2019)\n"
     ]
    }
   ],
   "source": [
    "kasus_test = [1,4,7,8,21,25,30]\n",
    "kasus_train = [i for i in range(len(X)) if i not in kasus_test]\n",
    "\n",
    "X_train, X_val  = X[kasus_train], X[kasus_test]\n",
    "y_train, y_val = y[kasus_train], y[kasus_test]\n",
    "\n",
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3092948-624b-4ae1-addf-684ecc3b7dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for n,i in enumerate(kasus_test):\n",
    "    data[f'Kasus {i}'] = X_val[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d945e9cb-d18f-4169-92e1-cf5e56c1fc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Dump the object into a pickle file\n",
    "with open('Kasus Validasi ML2.pkl', 'wb') as file:\n",
    "    pickle.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51b48f6c-6b71-4359-b5ff-13c7e89a3bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess data input\n",
    "X_train = torch.tensor(X_train, dtype = torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype = torch.float32)\n",
    "\n",
    "y_train = torch.tensor(y_train, dtype = torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype = torch.float32)\n",
    "\n",
    "n_train,width,height = y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "072db914-d040-4569-a60a-b08029c5b97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_data_for_fully_connected(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    function to process data ready for fully connected model\n",
    "    \"\"\"\n",
    "    n_train = len(X_train)\n",
    "    n_val = len(X_val)\n",
    "    X_train = X_train.view(n_train, -1)\n",
    "    y_train = y_train.view(n_train, -1)\n",
    "    X_val = X_val.view(n_val, -1)\n",
    "    y_val = y_val.view(n_val, -1)\n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a06b06cd-3103-4a70-825b-c5733bcf966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val = pre_process_data_for_fully_connected(X_train=X_train, y_train=y_train, \n",
    "                                                                      X_val=X_val, y_val=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6195805-2370-489b-9c90-dc0f67390d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 72]) torch.Size([7, 72]) torch.Size([30, 6214482]) torch.Size([7, 6214482])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0849c16b-b879-4f08-b998-55e6dd573580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_data_for_CNN_SBKabir(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Function to proses data ready for CNN SBKabir\n",
    "    \"\"\"\n",
    "    n_train, features = X_train.shape\n",
    "    n_val, features = X_val.shape\n",
    "\n",
    "    X_train = X_train.view(n_train, features, 1)\n",
    "    X_val = X_val.view(n_val, features,1)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f751972-26da-44d8-a4af-fc8c0278b6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "def create_dataloader(X, y, batch_size, shuffle=False):\n",
    "    dataset = CustomDataset(X, y)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return dataloader\n",
    "    \n",
    "# # Data loaders\n",
    "# batch_size = 2\n",
    "\n",
    "# train_loader_model1 = create_dataloader(X=X_train, y=y_train, batch_size=batch_size, shuffle=True)\n",
    "# val_loader_model1 = create_dataloader(X=X_val, y=y_val, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a855219e-8b14-4dd6-a934-cfd49a28c26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, optimizer, criterion, epochs, device):\n",
    "    model.train()\n",
    "    train_error, ssim_val, mse_val = [], [], []\n",
    "    for epoch in range(epochs):\n",
    "        for X,y in train_loader:\n",
    "            X,y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X)\n",
    "            loss = criterion(output,y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        actual, predicted = evaluate_model(model, val_loader)\n",
    "        ssim_values, mse_values = evaluate_flood_depth(actual, predicted)\n",
    "        train_error.append(loss.item()); ssim_val.append(np.mean(ssim_values)); mse_val.append(np.mean(mse_values))\n",
    "        print(f\"Epoch {epoch+1} Train MSE  is {loss.item()}, Val MSE {np.mean(mse_values)}, Val SSIM {np.mean(ssim_values)}\")\n",
    "    return  model, train_error, ssim_val, mse_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d3d23a-7944-40ad-8977-bf3dde73afbd",
   "metadata": {},
   "source": [
    "### Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "831486a2-bea5-494f-bef6-6cd9819cf33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import calculate_ssim,calculate_mse,evaluate_flood_depth,evaluate_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fbe16f-b818-4931-a395-4c74cf90dcb3",
   "metadata": {},
   "source": [
    "### Model CNN SBKabir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de51fe53-3ff0-45d9-9ba5-fc4c6fcc4f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class CNNModelBN(nn.Module):\n",
    "    def __init__(self, steps, features, outputs):\n",
    "        super(CNNModelBN, self).__init__()\n",
    "        \n",
    "        # Adjust in_channels to 72 based on the provided weight shape\n",
    "        self.conv1 = nn.Conv1d(in_channels=features, out_channels=64, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=256, kernel_size=1)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Adjust the input dimension of fc1 to 256 based on the provided weight shape\n",
    "        self.fc1 = nn.Linear(256 * steps, 256)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.bn4 = nn.BatchNorm1d(256)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc3 = nn.Linear(256, 64)\n",
    "        self.bn5 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        # Adjust fc4 to match the output dimensions of your provided weight shape\n",
    "        self.fc4 = nn.Linear(64, outputs)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4352fd7-2595-4138-a218-dde093bc184a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 72, 1]) torch.Size([7, 72, 1]) torch.Size([30, 6214482]) torch.Size([7, 6214482])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35de5025-66ac-4f97-879a-a64e30d55b35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val = pre_process_data_for_CNN_SBKabir(X_train, y_train, X_val, y_val)\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "train_loader_model_kabir = create_dataloader(X=X_train, y=y_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader_model_kabir = create_dataloader(X=X_val, y=y_val, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84e67f1a-12ee-471a-a861-e0f4f2174951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_ml2(input_size, output_size):\n",
    "    steps = 1\n",
    "    model = CNNModelBN(steps=steps, features=input_size, outputs=output_size)\n",
    "    return model\n",
    "\n",
    "def load_model_ml2(input_size, output_size, path_model):\n",
    "    model = create_model_ml2(input_size, output_size)\n",
    "    # Load the saved weights into the model\n",
    "    model.load_state_dict(torch.load(path_model))\n",
    "    model.eval()\n",
    "    print(\"Successfully  loaded ml2\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d84cf01d-e794-4f63-a80a-5873f8871252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully  loaded ml2\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1                [-1, 64, 1]           4,672\n",
      "       BatchNorm1d-2                [-1, 64, 1]             128\n",
      "              ReLU-3                [-1, 64, 1]               0\n",
      "            Conv1d-4               [-1, 256, 1]          16,640\n",
      "       BatchNorm1d-5               [-1, 256, 1]             512\n",
      "              ReLU-6               [-1, 256, 1]               0\n",
      "           Flatten-7                  [-1, 256]               0\n",
      "            Linear-8                  [-1, 256]          65,792\n",
      "       BatchNorm1d-9                  [-1, 256]             512\n",
      "             ReLU-10                  [-1, 256]               0\n",
      "          Dropout-11                  [-1, 256]               0\n",
      "           Linear-12                  [-1, 256]          65,792\n",
      "      BatchNorm1d-13                  [-1, 256]             512\n",
      "             ReLU-14                  [-1, 256]               0\n",
      "          Dropout-15                  [-1, 256]               0\n",
      "           Linear-16                   [-1, 64]          16,448\n",
      "      BatchNorm1d-17                   [-1, 64]             128\n",
      "             ReLU-18                   [-1, 64]               0\n",
      "           Linear-19              [-1, 6214482]     403,941,330\n",
      "================================================================\n",
      "Total params: 404,112,466\n",
      "Trainable params: 404,112,466\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 47.44\n",
      "Params size (MB): 1541.57\n",
      "Estimated Total Size (MB): 1589.01\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "#_, width, height = y_train.shape\n",
    "output_size = int(width * height)\n",
    "features = 72\n",
    "steps = 1\n",
    "\n",
    "#model = CNNModelBN(steps, features, output_size).to(device)\n",
    "path_pretrained = \"Improved CNN Kabir ML2 100 epoch 37 kasus.pth\"\n",
    "model = load_model_ml2(input_size=features, output_size=output_size, path_model=path_pretrained)\n",
    "model.to(device)\n",
    "# define hyperparameter\n",
    "lr = 0.001\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr)\n",
    "epochs = 2\n",
    "\n",
    "# Summarize the model\n",
    "summary(model, input_size=(72, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b51d7e2-428f-4f59-b2c3-e8e954714e03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train MSE  is 0.07964055240154266, Val MSE 0.034770760319424235, Val SSIM 0.9060371971761334\n",
      "Epoch 2 Train MSE  is 0.025861844420433044, Val MSE 0.02361006697056631, Val SSIM 0.8700108657885919\n"
     ]
    }
   ],
   "source": [
    "model, train_error, ssim_val, mse_val = train(model=model, train_loader=train_loader_model_kabir,val_loader= val_loader_model_kabir, optimizer=optimizer,\n",
    "                            criterion=criterion, epochs = epochs, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d3cd72-16be-4f2d-9b07-d5f90f320bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model state\n",
    "# torch.save(model.state_dict(), 'CNN Kabir 100 epoch run.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1378381d-43d4-487d-9eed-e23c32af15c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_training_error(mse_val, filename =\"validation_error_mse_cnn_kabir.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73442fab-8bf2-4114-9b39-2f11a775abd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_val = y_val.view(-1, width, height)\n",
    "# y_val = y_val.numpy()\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     predicted_val = model(X_val.to(device))\n",
    "#     predicted_val = predicted_val.view(-1, width,height)\n",
    "#     predicted_val = predicted_val.to(\"cpu\").numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8720a10-d2b1-4949-9fd6-0c29c7de3531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import convert_array_to_tif\n",
    "\n",
    "\n",
    "# filename = [\"Predicted kasus 6.tif\", \"Predicted kasus 7.tif\"]\n",
    "\n",
    "# for n,file in enumerate(filename):\n",
    "#     convert_array_to_tif(predicted_val[n], filename[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efd0458-a54c-40cb-931b-af9c40bee121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 0\n",
    "# compare_images(n=n, predicted= predicted_val[n], ground_truth=y_val[n], save= True, title='Kabir Comparison of Predicted and Ground Truth Images')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f80ab0-6bba-420d-8805-d38fecab6aed",
   "metadata": {},
   "source": [
    "### Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6f66062-5260-44eb-9868-d30bec4a953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleFC(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(SimpleFC, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd8a225e-4ab6-40bb-bb9b-de5f2d069140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create an array of 100 random values\n",
    "random_values = np.random.rand(1,100)\n",
    "random_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ceae85f1-6d8f-4002-91e2-8c1a5958e907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16416"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "144 * 114 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ee05cde-a227-48f0-91c3-73cd42238588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                [-1, 1, 64]           4,672\n",
      "              ReLU-2                [-1, 1, 64]               0\n",
      "            Linear-3                [-1, 1, 64]           4,160\n",
      "              ReLU-4                [-1, 1, 64]               0\n",
      "            Linear-5           [-1, 1, 6214482]     403,941,330\n",
      "              ReLU-6           [-1, 1, 6214482]               0\n",
      "================================================================\n",
      "Total params: 403,950,162\n",
      "Trainable params: 403,950,162\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 94.83\n",
      "Params size (MB): 1540.95\n",
      "Estimated Total Size (MB): 1635.78\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#define modele\n",
    "from torchsummary import summary\n",
    "input_size = 72\n",
    "output_size = width * height\n",
    "model = SimpleFC(input_size = input_size, output_size = output_size).to(device)\n",
    "# define hyperparameter\n",
    "lr = 0.001\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr)\n",
    "epochs = 200\n",
    "# Summarize the model\n",
    "summary(model, input_size=(1,72))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2cd402b6-7e1f-48fe-80fe-6277ea6a2421",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader_model1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, train_error, ssim_val, mse_val \u001b[38;5;241m=\u001b[39m train(model\u001b[38;5;241m=\u001b[39mmodel, train_loader\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_loader_model1\u001b[49m,val_loader\u001b[38;5;241m=\u001b[39m val_loader_model1, optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m      2\u001b[0m                             criterion\u001b[38;5;241m=\u001b[39mcriterion, epochs \u001b[38;5;241m=\u001b[39m epochs, device \u001b[38;5;241m=\u001b[39m device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader_model1' is not defined"
     ]
    }
   ],
   "source": [
    "model, train_error, ssim_val, mse_val = train(model=model, train_loader=train_loader_model1,val_loader= val_loader_model1, optimizer=optimizer,\n",
    "                            criterion=criterion, epochs = epochs, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d445365-92c7-46f9-b6d9-efab2f988860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model state\n",
    "# torch.save(model.state_dict(), 'Simple FC ML2 100 epoch run 2.pth')\n",
    "\n",
    "#load model \n",
    "# Load the saved model weights\n",
    "model.load_state_dict(torch.load('Simple FC ML2 100 epoch run 2.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28319d04-6bdf-46a0-9b9e-0678510e0d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_error(train_errors, title='Training Error over Epochs', filename='training_error_plot.png'):\n",
    "    \"\"\"\n",
    "    Plot the training errors over epochs and save the plot to a file.\n",
    "\n",
    "    Parameters:\n",
    "    - train_errors: A list of training error values.\n",
    "    - title: A string to title the plot (optional).\n",
    "    - filename: The filename to save the plot to (optional).\n",
    "    \"\"\"\n",
    "    # Number of epochs is the length of the train_errors list\n",
    "    epochs = range(1, len(train_errors) + 1)\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, train_errors, label='Training Error', linestyle='-')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # Save the plot\n",
    "    plt.savefig(filename)\n",
    "    plt.close()  # Close the plot window to free memory\n",
    "\n",
    "    print(f\"Plot saved as {filename}\")\n",
    "\n",
    "#plot_training_error(train_error, filename =\"training_error_mse_plot_simple_fc_run2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564da950-1271-46e5-b591-c19f67ed6a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    predicted_val = model(X_val.to(device))\n",
    "    predicted_val = predicted_val.view(-1, width,height)\n",
    "    predicted_val = predicted_val.to(\"cpu\").numpy()\n",
    "\n",
    "# y_val = y_val.view(-1, width, height)\n",
    "# y_val = y_val.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0523cb-70ef-4055-a439-28c8f201af1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import convert_array_to_tif\n",
    "\n",
    "\n",
    "filename = [\"FC Sederhana Predicted kasus 6.tif\", \"FC Sederhana Predicted kasus 7.tif\"]\n",
    "\n",
    "for n,file in enumerate(filename):\n",
    "    convert_array_to_tif(predicted_val[n], filename[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dcd66c-49f1-44b7-857d-e5fca7f5cb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def compare_images(n, predicted, ground_truth,save, title='Comparison of Predicted and Ground Truth Images'):\n",
    "    \"\"\"\n",
    "    This function takes two images, 'predicted' and 'ground_truth', and plots them side by side with a color bar indicating the range of pixel values.\n",
    "    \n",
    "    Parameters:\n",
    "    - predicted: The predicted image as a numpy array.\n",
    "    - ground_truth: The ground truth image as a numpy array.\n",
    "    - title: A string that represents the title of the plot.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 8))\n",
    "    \n",
    "    # Display predicted image\n",
    "    img1 = ax[0].imshow(predicted, aspect='equal', cmap = \"gray_r\")\n",
    "    ax[0].title.set_text('Predicted Image')\n",
    "    ax[0].axis('off')\n",
    "    \n",
    "    # Display ground truth image\n",
    "    img2 = ax[1].imshow(ground_truth, aspect='equal', cmap = \"gray_r\")\n",
    "    ax[1].title.set_text('Ground Truth Image')\n",
    "    ax[1].axis('off')\n",
    "    \n",
    "    # Add a color bar\n",
    "    fig.colorbar(img1, ax=ax, orientation='vertical', fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # Set the main title\n",
    "    plt.suptitle(title)\n",
    "    \n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust layout to make room for the main title\n",
    "    if save:\n",
    "        plt.savefig(f\"900Reverse Comparison of Predicted and actual flood depth with bar {n}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8360e85c-0e53-406e-a63b-9f0a65076c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1\n",
    "compare_images(n=n, predicted= predicted_val[n], ground_truth=y_val[n], save= True, title='Comparison of Predicted and Ground Truth Images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5246130-2799-4f22-9589-712d19c6d6ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
